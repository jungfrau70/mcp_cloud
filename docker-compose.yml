# Docker Compose configuration for production
services:
  
  # Neo4j Graph Database for RAG (✅ 신규 추가)
  mcp_neo4j:
    image: neo4j:5
    container_name: mcp_neo4j
    env_file: ./backend/env/.neo4j.env
    ports:
      - "7474:7474" # Neo4j Browser UI
      - "7687:7687" # Bolt protocol for application access
    volumes:
      - neo4j_data:/data
    networks:
      - mcp_cloud_network
    healthcheck:
      test: ["CMD-SHELL", "USER=$(echo \"$${NEO4J_AUTH}\" | cut -d'/' -f1); PASS=$(echo \"$${NEO4J_AUTH}\" | cut -d'/' -f2); cypher-shell -a bolt://localhost:7687 -u \"$$USER\" -p \"$$PASS\" 'RETURN 1' > /dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

  # PostgreSQL Database
  mcp_postgres:
    image: postgres:14
    container_name: mcp_postgres
    env_file: ./backend/env/.env
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - mcp_cloud_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mcpuser -d mcp_db"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  # OpenTelemetry Collector (receives OTLP gRPC/HTTP)
  otel-collector:
    image: otel/opentelemetry-collector:0.104.0
    container_name: otel_collector
    command: ["--config=/etc/otelcol/config.yaml", "--set=service.telemetry.logs.level=warn"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    networks:
      - mcp_cloud_network

  # Redis for caching
  mcp_redis:
    image: redis:7-alpine
    container_name: mcp_redis
    ports:
      - "6379:6379"
    networks:
      - mcp_cloud_network
  
  # Backend with package/app separation
  mcp_backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.app
    container_name: mcp_backend
    env_file: ./backend/env/.env
    expose:
      - "8000"
    environment:
      - REDIS_URL=redis://mcp_redis:6379
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      - GOOGLE_APPLICATION_CREDENTIALS=/app/gcp-sa-key.json
      - CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE=/app/gcp-sa-key.json
      # HF offline-friendly defaults
      - EMBEDDING_BACKEND=${EMBEDDING_BACKEND:-local}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-jhgan/ko-sroberta-multitask}
      - HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-1}
      - HF_LOCAL_FILES_ONLY=${HF_LOCAL_FILES_ONLY:-1}
      - HF_HOME=/cache/hf
      # 파일 감시 설정 개선
      - WATCHFILES_FORCE_POLLING=true
      - WATCHFILES_IGNORE_PATTERNS=*.db,*.db-journal,*.sqlite,*.sqlite-journal
      # Neo4j 연동 환경 변수 (✅ 신규 추가)
      - NEO4J_URI=bolt://mcp_neo4j:7687
    volumes:
      - ./backend:/app
      - ./backend/tests:/app/tests
      - terraform_modules_data:/app/terraform_modules
      - ./backend/env/alpha-ktixap-43e9bf90eb00.json:/app/gcp-sa-key.json
      - ./mcp_knowledge_base:/mcp_knowledge_base
      - hf_cache:/cache/hf
      - azure_config:/root/.azure
      - aws_config:/root/.aws
      - gcloud_config:/root/.config/gcloud
    depends_on:
      mcp_postgres:
        condition: service_healthy
      mcp_redis:
        condition: service_started
      mcp_neo4j:
        condition: service_healthy

    networks:
      - mcp_cloud_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # Celery worker (plan/apply/test)
  mcp_celery:
    build:
      context: ./backend
      dockerfile: Dockerfile.app
    container_name: mcp_celery
    env_file: ./backend/env/.env
    environment:
      - CELERY_BROKER_URL=redis://mcp_redis:6379/0
      - CELERY_RESULT_BACKEND=redis://mcp_redis:6379/0
      - OTEL_SERVICE_NAME=mcp-backend
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
    volumes:
      - ./backend:/app
      - terraform_modules_data:/app/terraform_modules
      - azure_config:/root/.azure
      - aws_config:/root/.aws
      - gcloud_config:/root/.config/gcloud
    depends_on:
      mcp_redis:
        condition: service_started
      otel-collector:
        condition: service_started
    command: ["sh", "-lc", "python -m celery -A worker.celery_app worker -Q plan,apply,test -c 4 -l warning"]
    healthcheck:
      disable: true
    networks:
      - mcp_cloud_network

  # Optional: Celery beat (enable if scheduled tasks are defined)
  mcp_celery_beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.app
    container_name: mcp_celery_beat
    env_file: ./backend/env/.env
    environment:
      - CELERY_BROKER_URL=redis://mcp_redis:6379/0
      - CELERY_RESULT_BACKEND=redis://mcp_redis:6379/0
      - OTEL_SERVICE_NAME=mcp-backend
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
    volumes:
      - ./backend:/app
    depends_on:
      mcp_redis:
        condition: service_started
    command: ["sh", "-lc", "python -m celery -A worker.celery_app beat -l warning"]
    healthcheck:
      disable: true
    profiles: ["scheduler"]
    networks:
      - mcp_cloud_network
    
  # Frontend with package/app separation
  mcp_frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.app
    container_name: mcp_frontend
    # 프론트엔드는 민감키 불필요: env_file 제거
    environment:
      - NUXT_PUBLIC_API_BASE_URL=/api
      - NODE_ENV=development
      - CHOKIDAR_USEPOLLING=true
      - NUXT_TELEMETRY_DISABLED=1
    ports:
      - "3000:3000"
    expose:
      - "3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    command: ["sh", "-lc", "rm -rf .nuxt .output && yarn dev --host 0.0.0.0 --port 3000"]
    depends_on:
      - mcp_backend
    networks:
      - mcp_cloud_network

  # Nginx Proxy Manager (내부 프록시, 외부 포트 미개방)
  npm:
    image: jc21/nginx-proxy-manager:latest
    container_name: mcp_npm
    env_file:
      - ./.env
    expose:
      - "80"   # HTTP (내부에서만 노출)
      - "81"   # Admin UI (내부에서만 노출)
      - "443"  # HTTPS (내부에서만 노출)
    volumes:
      - npm_data:/data
      - npm_letsencrypt:/etc/letsencrypt
    depends_on:
      - mcp_frontend
    networks:
      - mcp_cloud_network

  # Authelia (SSO/2FA 게이트웨이)
  authelia:
    image: authelia/authelia:latest
    container_name: mcp_authelia
    env_file:
      - ./.env
    expose:
      - "9091"
    volumes:
      - ./authelia/config:/config
    command: ["--config", "/config/configuration.yml"]
    networks:
      - mcp_cloud_network

  # Cloudflare Tunnel (외부 → NPM 프록시, 호스트 포트 불필요)
  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    env_file:
      - ./.env
    command: ["--config", "/etc/cloudflared/config.yml", "--no-autoupdate", "tunnel", "run"]
    volumes:
      - ./cloudflared/config/config.yml:/etc/cloudflared/config.yml:ro
      - ./cloudflared/config/config.yml:/home/nonroot/.cloudflared/config.yml:ro
      - ./cloudflared/data:/home/nonroot/.cloudflared
    depends_on:
      - npm
    networks:
      - mcp_cloud_network

volumes:
  postgres_data:
  terraform_modules_data:
  hf_cache:
  azure_config:
  npm_data:
  npm_letsencrypt:
  aws_config:
  gcloud_config:
  neo4j_data:

networks:
  mcp_cloud_network:
    driver: bridge